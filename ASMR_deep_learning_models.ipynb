{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a600f9a5",
   "metadata": {},
   "source": [
    "\n",
    "# ASMR: App for Safe & Emotion Matter Right\n",
    "\n",
    "This notebook implements the core functionality of ASMR, focusing on:\n",
    "1. Speech Emotion Recognition using a 1D-CNN.\n",
    "2. Nonverbal Sound Recognition using a Capsule Network (CapsNet).\n",
    "\n",
    "Both models use deep learning to improve accessibility for individuals with hearing impairments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install necessary libraries (uncomment if needed)\n",
    "# !pip install tensorflow librosa numpy scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dataset preparation placeholder (to be replaced with actual dataset path)\n",
    "def load_audio_data(data_dir, sr=16000, duration=3):\n",
    "    '''\n",
    "    Load audio data and preprocess it for the models.\n",
    "    :param data_dir: Path to the directory containing audio files.\n",
    "    :param sr: Sampling rate.\n",
    "    :param duration: Duration of audio clips in seconds.\n",
    "    :return: Preprocessed audio data and labels.\n",
    "    '''\n",
    "    audio_data = []\n",
    "    labels = []\n",
    "    for label in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for file in os.listdir(class_dir):\n",
    "                try:\n",
    "                    audio_path = os.path.join(class_dir, file)\n",
    "                    y, _ = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "                    audio_data.append(y)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file}: {e}\")\n",
    "    return np.array(audio_data), np.array(labels)\n",
    "\n",
    "# Example usage (update 'data_dir' to actual dataset path)\n",
    "# data_dir = 'path_to_dataset'\n",
    "# audio_data, labels = load_audio_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a69897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_1d_cnn(input_shape, num_classes):\n",
    "    '''\n",
    "    Build a 1D-CNN for speech emotion recognition.\n",
    "    :param input_shape: Shape of the input data (e.g., (samples, features)).\n",
    "    :param num_classes: Number of emotion classes.\n",
    "    :return: Compiled CNN model.\n",
    "    '''\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example usage (replace 'input_shape' and 'num_classes' with actual values)\n",
    "# model = build_1d_cnn(input_shape=(16000, 1), num_classes=7)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ac1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_capsnet(input_shape, num_classes):\n",
    "    '''\n",
    "    Build a Capsule Network for nonverbal sound recognition.\n",
    "    :param input_shape: Shape of the input data (e.g., (samples, features)).\n",
    "    :param num_classes: Number of sound categories.\n",
    "    :return: CapsNet model.\n",
    "    '''\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    conv1 = layers.Conv1D(256, kernel_size=9, strides=1, activation='relu')(input_layer)\n",
    "    conv2 = layers.Conv1D(256, kernel_size=9, strides=2, activation='relu')(conv1)\n",
    "    \n",
    "    # Primary Capsule Layer\n",
    "    primary_caps = layers.Conv1D(32 * 8, kernel_size=9, strides=2, activation='relu')(conv2)\n",
    "    primary_caps = layers.Reshape([32, 8])(primary_caps)\n",
    "    \n",
    "    # Digit Capsule Layer\n",
    "    capsule_layer = layers.LSTM(16)(primary_caps)  # Simplified version for demo\n",
    "    \n",
    "    # Output Layer\n",
    "    output = layers.Dense(num_classes, activation='softmax')(capsule_layer)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example usage (replace 'input_shape' and 'num_classes' with actual values)\n",
    "# model = build_capsnet(input_shape=(16000, 1), num_classes=10)\n",
    "# model.summary()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
